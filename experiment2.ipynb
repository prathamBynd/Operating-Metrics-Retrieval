{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.pdf import partition_pdf\n",
    "import PyPDF2\n",
    "import anthropic\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import shutil\n",
    "import base64\n",
    "import pickle\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"ANTHROPIC_API_KEY\"]  = \"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\"\n",
    "\n",
    "client=anthropic.Anthropic()\n",
    "\n",
    "haiku = \"claude-3-haiku-20240307\"\n",
    "sonnet = \"claude-3-sonnet-20240229\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_company=\"Amazon.com, Inc.\"\n",
    "input_sector=\"E-commerce\"\n",
    "input_pdf=\"AmazonAnnualReport.pdf\"\n",
    "input_metric=\"income from cloud business\"\n",
    "input_year=\"2023\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pdf_elements = partition_pdf(\n",
    "#     input_pdf,\n",
    "#     chunking_strategy=\"by_title\",\n",
    "#     extract_images_in_pdf=False,\n",
    "#     infer_table_structure=True,\n",
    "#     max_characters=3000,\n",
    "#     new_after_n_chars=2800,\n",
    "#     combine_text_under_n_chars=2000\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open(\"apple_pdf_elements.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(pdf_elements, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"amazon_pdf_elements.pkl\", \"rb\") as f:\n",
    "    pdf_elements = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categorize_elements(raw_pdf_elements):\n",
    "    text_elements = []\n",
    "    table_elements = []\n",
    "    table_data=[]\n",
    "    for element in raw_pdf_elements:\n",
    "        if 'CompositeElement' in str(type(element)):\n",
    "            text_elements.append(element)\n",
    "        elif 'Table' in str(type(element)):\n",
    "            table_elements.append(element)\n",
    "            table_data.append(str(element))\n",
    "    return text_elements, table_elements, table_data\n",
    "\n",
    "texts, tables, tables_text = categorize_elements(pdf_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(message):\n",
    "    response = client2.messages.create(\n",
    "        model=haiku,\n",
    "        max_tokens=1024,\n",
    "        messages=message\n",
    "    )\n",
    "    return response.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response_sonnet(message):\n",
    "    response = client2.messages.create(\n",
    "        model=sonnet,\n",
    "        max_tokens=1024,\n",
    "        messages=message\n",
    "    )\n",
    "    return response.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages=[\n",
    "{\n",
    "    \"role\": \"user\",\n",
    "    \"content\": [\n",
    "    {\"type\": \"text\", \"text\": f'''You are provided with the following information about a specific metric used by a publicly listed company:\n",
    "\n",
    "    Metric Name: {input_metric}\n",
    "    Company Name: {input_company}\n",
    "    Sector of the Company: {input_sector}\n",
    "    \n",
    "    This metric or its variations are reported in the company's annual report. Please generate a list of 5 specific keywords to search for in the annual report. These phrases or keywords should capture potential syntactic and semantic variations of the metric name, based on its description and what you know about the company. The goal is to find the most relevant sections of the annual report with minimal irrelevant matches. Do not provide me business keywords like sales, units, shipment etc. Provide each keyword on a new line and do not include any special symbols or bullet points in your answer. All keywords should only be one word and not be more than that. A keyword shouldn't be a concatenation of two words like PhoneSales, TransportRevenue etc.'''}\n",
    "    ],\n",
    "}\n",
    "]\n",
    "\n",
    "response=get_response(messages)\n",
    "\n",
    "print(response)\n",
    "\n",
    "words = response.split()\n",
    "\n",
    "keywords = set()\n",
    "\n",
    "for word in words:\n",
    "    keywords.add(word.strip())\n",
    "\n",
    "keywords = list(keywords)\n",
    "\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Lowercase the text\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove special characters and digits\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    processed_text = ' '.join(tokens)\n",
    "    \n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "presentText=[]\n",
    "\n",
    "for text in texts:\n",
    "    text = preprocess_text(text)\n",
    "    present = False\n",
    "    for keyword in keywords:\n",
    "        keyword=preprocess_text(keyword)\n",
    "        if keyword in text:\n",
    "            present=True\n",
    "            break\n",
    "\n",
    "    if not present:\n",
    "        continue\n",
    "\n",
    "    presentText.append(text)\n",
    "\n",
    "\n",
    "presentTable=[]\n",
    "\n",
    "for text in tables_text:\n",
    "    text = preprocess_text(text)\n",
    "    frequency = 0\n",
    "    for keyword in keywords:\n",
    "        keyword=preprocess_text(keyword)\n",
    "        if keyword in text:\n",
    "            frequency += 1\n",
    "    frequency_list.append(frequency)\n",
    "\n",
    "pdf_tables = [text for text, freq in zip(pdf_tables, frequency_list) if freq != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_page(page_number, input_pdf=input_pdf, output_pdf=\"page.pdf\"):\n",
    "    with open(input_pdf, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        writer = PyPDF2.PdfWriter()\n",
    "\n",
    "        # Extract the page and add it to the writer\n",
    "        page = reader.pages[page_number-1]  # Page numbers start from 0\n",
    "        writer.add_page(page)\n",
    "\n",
    "        # Write the output PDF to a file\n",
    "        with open(output_pdf, 'wb') as output_file:\n",
    "            writer.write(output_file)\n",
    "\n",
    "def encode_image(image_path):\n",
    "  with open(image_path, \"rb\") as image_file:\n",
    "    return base64.b64encode(image_file.read()).decode('utf-8')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
